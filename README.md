# ğŸ“¡ Infrastructure IoT Intelligente : Pipeline de Streaming Temps RÃ©el

## ğŸ“ Description du Projet
Ce projet est une implÃ©mentation complÃ¨te d'un pipeline d'ingÃ©nierie des donnÃ©es (Data Engineering Pipeline) conÃ§u pour traiter des flux de donnÃ©es IoT en temps rÃ©el.

L'objectif est de simuler une flotte de capteurs connectÃ©s, d'ingÃ©rer leurs donnÃ©es Ã  haute frÃ©quence, et de les traiter Ã  la volÃ©e pour dÃ©tecter des anomalies critiques (ex: surchauffe) avant le stockage.

## ğŸ—ï¸ Architecture Technique
![Architecture Diagram](./diagrams/Architecture .png)

Le projet repose sur une architecture conteneurisÃ©e via **Docker** :
* **Source** : Script Python simulant des capteurs IoT (TempÃ©rature, HumiditÃ©, Geolocation) gÃ©nÃ©rant des donnÃ©es au format JSON.
* **Ingestion (Message Broker)** : **Apache Kafka** & Zookeeper pour gÃ©rer le flux de donnÃ©es Ã  haut dÃ©bit et dÃ©coupler les services.
* **Traitement (Stream Processing)** : **Apache Spark (Structured Streaming)** pour consommer les topics Kafka, parser les donnÃ©es JSON complexes et appliquer des rÃ¨gles mÃ©tier en temps rÃ©el (filtrage des tempÃ©ratures critiques > 80Â°C).
* **Stockage (Ã€ venir)** : MongoDB pour la persistance des donnÃ©es.
* **Visualisation (Ã€ venir)** : Grafana.

## ğŸ› ï¸ Stack Technologique
* **Langage** : Python 3.9 (PySpark, Kafka-Python)
* **Containerisation** : Docker & Docker Compose
* **Big Data** : Apache Kafka 7.4, Apache Spark 3.5
* **OS** : DÃ©veloppement sous Windows

## ğŸš€ Comment lancer le projet

### PrÃ©-requis
* Docker Desktop installÃ© et dÃ©marrÃ©.

### Installation
1.  Cloner le repo :
    ```bash
    git clone [https://github.com/TON_PSEUDO/iot-realtime-pipeline.git](https://github.com/TON_PSEUDO/iot-realtime-pipeline.git)
    ```
2.  Lancer l'infrastructure Docker :
    ```bash
    docker-compose up -d
    ```

### Utilisation
1.  **Lancer le gÃ©nÃ©rateur de donnÃ©es** (Simulateur IoT) :
    ```bash
    python scripts/iot_generator.py
    ```
2.  **Lancer le Job Spark** pour voir le traitement en direct :
    ```bash
    docker exec -it spark-master /opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 /app/scripts/spark_streaming.py
    ```

## ğŸ“¸ RÃ©sultats
Voici un aperÃ§u du traitement Spark en temps rÃ©el :
![Spark Output](./diagrams/spark_output.png)
*(Mets ici ta capture d'Ã©cran du terminal avec les tableaux)*

## ğŸ”„ Prochaines Ã©tapes
* [ ] Connecter le flux de sortie Spark vers MongoDB (Sink).
* [ ] CrÃ©er un dashboard de monitoring avec Grafana.
* [ ] DÃ©ployer sur le Cloud (AWS/Azure).
